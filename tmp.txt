'Conditional Variational Autoencoder with Adversarial Learning for\nEnd-to-End Text-to-Speech\n\nJaehyeon Kim 1 Jungil Kong 1 Juhee Son 1 2\n\n1\n2\n0\n2\n\nn\nu\nJ\n\n1\n1\n\n]\n\nD\nS\n.\ns\nc\n[\n\n1\nv\n3\n0\n1\n6\n0\n.\n6\n0\n1\n2\n:\nv\ni\nX\nr\na\n\nAbstract\n\nSeveral recent end-to-end text-to-speech (TTS)\nmodels enabling single-stage training and parallel\nsampling have been proposed, but their sample\nquality does not match that of two-stage TTS sys-\ntems.\nIn this work, we present a parallel end-\nto-end TTS method that generates more natu-\nral sounding audio than current two-stage mod-\nels. Our method adopts variational inference aug-\nmented with normalizing ﬂows and an adversarial\ntraining process, which improves the expressive\npower of generative modeling. We also propose a\nstochastic duration predictor to synthesize speech\nwith diverse rhythms from input text. With the\nuncertainty modeling over latent variables and\nthe stochastic duration predictor, our method ex-\npresses the natural one-to-many relationship in\nwhich a text input can be spoken in multiple ways\nwith different pitches and rhythms. A subjective\nhuman evaluation (mean opinion score, or MOS)\non the LJ Speech, a single speaker dataset, shows\nthat our method outperforms the best publicly\navailable TTS systems and achieves a MOS com-\nparable to ground truth.\n\n1. Introduction\n\nText-to-speech (TTS) systems synthesize raw speech wave-\nforms from given text through several components. With\nthe rapid development of deep neural networks, TTS sys-\ntem pipelines have been simpliﬁed to two-stage genera-\ntive modeling apart from text preprocessing such as text\nnormalization and phonemization. The ﬁrst stage is to\nproduce intermediate speech representations such as mel-\nspectrograms (Shen et al., 2018) or linguistic features (Oord\n\n1Kakao Enterprise, Seongnam-si, Gyeonggi-do, Repub-\nlic of Korea 2School of Computing, KAIST, Daejeon, Re-\npublic of Korea.\nJaehyeon Kim\n<jay.xyz@kakaoenterprise.com>.\n\nCorrespondence to:\n\nProceedings of the 38 th International Conference on Machine\nLearning, PMLR 139, 2021. Copyright 2021 by the author(s).\n\net al., 2016) from the preprocessed text,1 and the second\nstage is to generate raw waveforms conditioned on the in-\ntermediate representations (Oord et al., 2016; Kalchbrenner\net al., 2018). Models at each of the two-stage pipelines have\nbeen developed independently.\n\nNeural network-based autoregressive TTS systems have\nshown the capability of synthesizing realistic speech (Shen\net al., 2018; Li et al., 2019), but their sequential generative\nprocess makes it difﬁcult to fully utilize modern parallel pro-\ncessors. To overcome this limitation and improve synthesis\nspeed, several non-autoregressive methods have been pro-\nposed. In the text-to-spectrogram generation step, extracting\nattention maps from pre-trained autoregressive teacher net-\nworks (Ren et al., 2019; Peng et al., 2020) is attempted to\ndecrease the difﬁculty of learning alignments between text\nand spectrograms. More recently, likelihood-based methods\nfurther eliminate the dependency on external aligners by\nestimating or learning alignments that maximize the likeli-\nhood of target mel-spectrograms (Zeng et al., 2020; Miao\net al., 2020; Kim et al., 2020). Meanwhile, generative adver-\nsarial networks (GANs) (Goodfellow et al., 2014) have been\nexplored in second stage models. GAN-based feed-forward\nnetworks with multiple discriminators, each distinguishing\nsamples at different scales or periods, achieve high-quality\nraw waveform synthesis (Kumar et al., 2019; Bi´nkowski\net al., 2019; Kong et al., 2020).\n\nDespite the progress of parallel TTS systems, two-stage\npipelines remain problematic because they require sequen-\ntial training or ﬁne-tuning (Shen et al., 2018; Weiss et al.,\n2020) for high-quality production wherein latter stage mod-\nels are trained with the generated samples of earlier stage\nmodels. In addition, their dependency on predeﬁned inter-\nmediate features precludes applying learned hidden repre-\nsentations to obtain further improvements in performance.\nRecently, several works, i.e., FastSpeech 2s (Ren et al.,\n2021) and EATS (Donahue et al., 2021), have proposed\nefﬁcient end-to-end training methods such as training over\nshort audio clips rather than entire waveforms, leveraging a\nmel-spectrogram decoder to aid text representation learning,\n\n1Although there is a text preprocessing step in TTS systems,\nWe herein use preprocessed text interchangeably with the word\n“text”.\n\n \n \n \n \n \n \n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nand designing a specialized spectrogram loss to relax length-\nmismatch between target and generated speech. However,\ndespite potentially improving performance by utilizing the\nlearned representations, their synthesis quality lags behind\ntwo-stage systems.\n\nIn this work, we present a parallel end-to-end TTS method\nthat generates more natural sounding audio than cur-\nrent two-stage models. Using a variational autoencoder\n(VAE) (Kingma & Welling, 2014), we connect two modules\nof TTS systems through latent variables to enable efﬁcient\nend-to-end learning. To improve the expressive power of\nour method so that high-quality speech waveforms can be\nsynthesized, we apply normalizing ﬂows to our conditional\nprior distribution and adversarial training on the waveform\ndomain. In addition to generating ﬁne-grained audio, it is\nimportant for TTS systems to express the one-to-many rela-\ntionship in which text input can be spoken in multiple ways\nwith different variations (e.g., pitch and duration). To tackle\nthe one-to-many problem, we also propose a stochastic du-\nration predictor to synthesize speech with diverse rhythms\nfrom input text. With the uncertainty modeling over latent\nvariables and the stochastic duration predictor, our method\ncaptures speech variations that cannot be represented by\ntext.\n\nOur method obtains more natural sounding speech and\nhigher sampling efﬁciency than the best publicly avail-\nable TTS system, Glow-TTS (Kim et al., 2020) with HiFi-\nGAN (Kong et al., 2020). We make both our demo page\nand source-code publicly available.2\n\n2. Method\n\nIn this section, we explain our proposed method and the ar-\nchitecture of it. The proposed method is mostly described in\nthe ﬁrst three subsections: a conditional VAE formulation;\nalignment estimation derived from variational inference;\nadversarial training for improving synthesis quality. The\noverall architecture is described at the end of this section.\nFigures 1a and 1b show the training and inference proce-\ndures of our method, respectively. From now on, we will\nrefer to our method as Variational Inference with adversarial\nlearning for end-to-end Text-to-Speech (VITS).\n\n2.1. Variational Inference\n\n2.1.1. OVERVIEW\n\nVITS can be expressed as a conditional VAE with the ob-\njective of maximizing the variational lower bound, also\ncalled the evidence lower bound (ELBO), of the intractable\nmarginal log-likelihood of data log pθ(x|c):\n\n2Source-code: https://github.com/jaywalnut310/vits\nDemo: https://jaywalnut310.github.io/vits-demo/index.html\n\nlog pθ(x|c) ≥ Eqφ(z|x)\n\n(cid:104)\n\nlog pθ(x|z)−log\n\n(cid:105)\n\nqφ(z|x)\npθ(z|c)\n\n(1)\n\nwhere pθ(z|c) denotes a prior distribution of the latent vari-\nables z given condition c, pθ(x|z) is the likelihood func-\ntion of a data point x, and qφ(z|x) is an approximate pos-\nterior distribution. The training loss is then the negative\nELBO, which can be viewed as the sum of reconstruc-\ntion loss − log pθ(x|z) and KL divergence log qφ(z|x) −\nlog pθ(z|c), where z ∼ qφ(z|x).\n\n2.1.2. RECONSTRUCTION LOSS\n\nAs a target data point in the reconstruction loss, we use a\nmel-spectrogram instead of a raw waveform, denoted by\nxmel. We upsample the latent variables z to the waveform\ndomain ˆy through a decoder and transform ˆy to the mel-\nspectrogram domain ˆxmel. Then the L1 loss between the\npredicted and target mel-spectrogram is used as the recon-\nstruction loss:\n\nLrecon = (cid:107)xmel − ˆxmel(cid:107)1\n\n(2)\n\nThis can be viewed as maximum likelihood estimation as-\nsuming a Laplace distribution for the data distribution and\nignoring constant terms. We deﬁne the reconstruction loss in\nthe mel-spectrogram domain to improve the perceptual qual-\nity by using a mel-scale that approximates the response of\nthe human auditory system. Note that the mel-spectrogram\nestimation from a raw waveform does not require trainable\nparameters as it only uses STFT and linear projection onto\nthe mel-scale. Furthermore, the estimation is only employed\nduring training, not inference. In practice, we do not upsam-\nple the whole latent variables z but use partial sequences as\nan input for the decoder, which is the windowed generator\ntraining used for efﬁcient end-to-end training (Ren et al.,\n2021; Donahue et al., 2021).\n\n2.1.3. KL-DIVERGENCE\n\nThe input condition of the prior encoder c is composed of\nphonemes ctext extracted from text and an alignment A be-\ntween phonemes and latent variables. The alignment is a\nhard monotonic attention matrix with |ctext| × |z| dimen-\nsions representing how long each input phoneme expands to\nbe time-aligned with the target speech. Because there are no\nground truth labels for the alignment, we must estimate the\nalignment at each training iteration, which we will discuss\nin Section 2.2.1. In our problem setting, we aim to provide\nmore high-resolution information for the posterior encoder.\nWe, therefore, use the linear-scale spectrogram of target\nspeech xlin as input rather than the mel-spectrogram. Note\nthat the modiﬁed input does not violate the properties of\nvariational inference. The KL divergence is then:\n\nLkl = log qφ(z|xlin) − log pθ(z|ctext, A),\nz ∼ qφ(z|xlin) = N (z; µφ(xlin), σφ(xlin))\n\n(3)\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\n(a) Training procedure\n\n(b) Inference procedure\n\nFigure 1. System diagram depicting (a) training procedure and (b) inference procedure. The proposed model can be viewed as a conditional\nVAE; a posterior encoder, decoder, and conditional prior (green blocks: a normalizing ﬂow, linear projection layer, and text encoder) with\na ﬂow-based stochastic duration predictor.\n\nThe factorized normal distribution is used to parameterize\nour prior and posterior encoders. We found that increasing\nthe expressiveness of the prior distribution is important for\ngenerating realistic samples. We, therefore, apply a normal-\nizing ﬂow fθ (Rezende & Mohamed, 2015), which allows\nan invertible transformation of a simple distribution into a\nmore complex distribution following the rule of change-of-\nvariables, on top of the factorized normal prior distribution:\n\ntext in order without skipping any words. To ﬁnd the\noptimum alignment, Kim et al. (2020) use dynamic pro-\ngramming. Applying MAS directly in our setting is dif-\nﬁcult because our objective is the ELBO, not the exact\nlog-likelihood. We, therefore, redeﬁne MAS to ﬁnd an\nalignment that maximizes the ELBO, which reduces to ﬁnd-\ning an alignment that maximizes the log-likelihood of the\nlatent variables z:\n\npθ(z|c) = N (fθ(z); µθ(c), σθ(c))\n\nc = [ctext, A]\n\n(cid:12)\n(cid:12)\n(cid:12) det\n\n∂fθ(z)\n∂z\n\n(cid:12)\n(cid:12)\n(cid:12),\n\n(4)\n\narg max\nˆA\n\nlog pθ(xmel|z) − log\n\nqφ(z|xlin)\npθ(z|ctext, ˆA)\n\n2.2. Alignment Estimation\n\n2.2.1. MONOTONIC ALIGNMENT SEARCH\n\nTo estimate an alignment A between input\ntext and\ntarget speech, we adopt Monotonic Alignment Search\n(MAS) (Kim et al., 2020), a method to search an align-\nment that maximizes the likelihood of data parameterized\nby a normalizing ﬂow f :\n\nA = arg max\n\nˆA\n\n= arg max\n\nˆA\n\nlog p(x|ctext, ˆA)\n\nlog N (f (x); µ(ctext, ˆA), σ(ctext, ˆA)) (5)\n\nwhere the candidate alignments are restricted to be mono-\ntonic and non-skipping following the fact that humans read\n\n= arg max\n\nˆA\n\nlog pθ(z|ctext, ˆA)\n\n= log N (fθ(z); µθ(ctext, ˆA), σθ(ctext, ˆA))\n\n(6)\n\nDue to the resemblance of Equation 5 to Equation 6, we can\nuse the original MAS implementation without modiﬁcation.\nAppendix A includes pseudocode for MAS.\n\n2.2.2. DURATION PREDICTION FROM TEXT\n\nWe can calculate the duration of each input token di by sum-\nming all the columns in each row of the estimated alignment\n(cid:80)\nj Ai,j. The duration could be used to train a determinis-\ntic duration predictor, as proposed in previous work (Kim\net al., 2020), but it cannot express the way a person utters at\ndifferent speaking rates each time. To generate human-like\nrhythms of speech, we design a stochastic duration predictor\nso that its samples follow the duration distribution of given\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nphonemes. The stochastic duration predictor is a ﬂow-based\ngenerative model that is typically trained via maximum like-\nlihood estimation. The direct application of maximum likeli-\nhood estimation, however, is difﬁcult because the duration of\neach input phoneme is 1) a discrete integer, which needs to\nbe dequantized for using continuous normalizing ﬂows, and\n2) a scalar, which prevents high-dimensional transformation\ndue to invertibility. We apply variational dequantization (Ho\net al., 2019) and variational data augmentation (Chen et al.,\n2020) to solve these problems. To be speciﬁc, we introduce\ntwo random variables u and ν, which have the same time\nresolution and dimension as that of the duration sequence d,\nfor variational dequatization and variational data augmenta-\ntion, respectively. We restrict the support of u to be [0, 1) so\nthat the difference d − u becomes a sequence of positive real\nnumbers, and we concatenate ν and d channel-wise to make\na higher dimensional latent representation. We sample the\ntwo variables through an approximate posterior distribution\nqφ(u, ν|d, ctext). The resulting objective is a variational\nlower bound of the log-likelihood of the phoneme duration:\n\nlog pθ(d|ctext) ≥\n\nEqφ(u,ν|d,ctext)\n\n(cid:104)\n\nlog\n\npθ(d − u, ν|ctext)\nqφ(u, ν|d, ctext)\n\n(cid:105)\n\n(7)\n\nThe training loss Ldur is then the negative variational lower\nbound. We apply the stop gradient operator (van den Oord\net al., 2017), which prevents back-propagating the gradient\nof inputs, to the input conditions so that the training of the\nduration predictor does not affect that of other modules.\n\nThe sampling procedure is relatively simple; the phoneme\nduration is sampled from random noise through the inverse\ntransformation of the stochastic duration predictor, and then\nit is converted to integers.\n\n2.3. Adversarial Training\n\nTo adopt adversarial training in our learning system, we add\na discriminator D that distinguishes between the output gen-\nerated by the decoder G and the ground truth waveform y.\nIn this work, we use two types of loss successfully applied in\nspeech synthesis; the least-squares loss function (Mao et al.,\n2017) for adversarial training, and the additional feature-\nmatching loss (Larsen et al., 2016) for training the generator:\n\nLadv(D) = E(y,z)\n\n(cid:104)\n\n(D(y) − 1)2 + (D(G(z)))2(cid:105)\n\n,\n\nLadv(G) = Ez\n\n(cid:104)\n\n(D(G(z)) − 1)2(cid:105)\n\n,\n\n(8)\n\n(9)\n\nLf m(G) = E(y,z)\n\n(cid:104) T\n(cid:88)\n\nl=1\n\n1\nNl\n\n(cid:107)Dl(y) − Dl(G(z))(cid:107)1\n\n(cid:105)\n\n(10)\n\nthe discriminator with Nl number of features. Notably, the\nfeature matching loss can be seen as reconstruction loss that\nis measured in the hidden layers of the discriminator sug-\ngested as an alternative to the element-wise reconstruction\nloss of VAEs (Larsen et al., 2016).\n\n2.4. Final Loss\n\nWith the combination of VAE and GAN training, the total\nloss for training our conditional VAE can be expressed as\nfollows:\n\nLvae = Lrecon + Lkl + Ldur + Ladv(G) + Lf m(G) (11)\n\n2.5. Model Architecture\n\nThe overall architecture of the proposed model consists of\na posterior encoder, prior encoder, decoder, discriminator,\nand stochastic duration predictor. The posterior encoder and\ndiscriminator are only used for training, not for inference.\nArchitectural details are available in Appendix B.\n\n2.5.1. POSTERIOR ENCODER\n\nFor the posterior encoder, we use the non-causal WaveNet\nresidual blocks used in WaveGlow (Prenger et al., 2019)\nand Glow-TTS (Kim et al., 2020). A WaveNet residual\nblock consists of layers of dilated convolutions with a gated\nactivation unit and skip connection. The linear projection\nlayer above the blocks produces the mean and variance of\nthe normal posterior distribution. For the multi-speaker case,\nwe use global conditioning (Oord et al., 2016) in residual\nblocks to add speaker embedding.\n\n2.5.2. PRIOR ENCODER\n\nThe prior encoder consists of a text encoder that processes\nthe input phonemes ctext and a normalizing ﬂow fθ that\nimproves the ﬂexibility of the prior distribution. The text\nencoder is a transformer encoder (Vaswani et al., 2017) that\nuses relative positional representation (Shaw et al., 2018)\ninstead of absolute positional encoding. We can obtain the\nhidden representation htext from ctext through the text en-\ncoder and a linear projection layer above the text encoder\nthat produces the mean and variance used for constructing\nthe prior distribution. The normalizing ﬂow is a stack of\nafﬁne coupling layers (Dinh et al., 2017) consisting of a\nstack of WaveNet residual blocks. For simplicity, we design\nthe normalizing ﬂow to be a volume-preserving transforma-\ntion with the Jacobian determinant of one. For the multi-\nspeaker setting, we add speaker embedding to the residual\nblocks in the normalizing ﬂow through global conditioning.\n\n2.5.3. DECODER\n\nwhere T denotes the total number of layers in the discrim-\ninator and Dl outputs the feature map of the l-th layer of\n\nThe decoder is essentially the HiFi-GAN V1 genera-\ntor (Kong et al., 2020). It is composed of a stack of trans-\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nposed convolutions, each of which is followed by a multi-\nreceptive ﬁeld fusion module (MRF). The output of the\nMRF is the sum of the output of residual blocks that have\ndifferent receptive ﬁeld sizes. For the multi-speaker setting,\nwe add a linear layer that transforms speaker embedding\nand add it to the input latent variables z.\n\n2.5.4. DISCRIMINATOR\n\nWe follow the discriminator architecture of the multi-period\ndiscriminator proposed in HiFi-GAN (Kong et al., 2020).\nThe multi-period discriminator is a mixture of Markovian\nwindow-based sub-discriminators (Kumar et al., 2019), each\nof which operates on different periodic patterns of input\nwaveforms.\n\n3.2. Preprocessing\n\nWe use linear spectrograms which can be obtained from\nraw waveforms through the Short-time Fourier transform\n(STFT), as input of the posterior encoder. The FFT size,\nwindow size and hop size of the transform are set to 1024,\n1024 and 256, respectively. We use 80 bands mel-scale\nspectrograms for reconstruction loss, which is obtained by\napplying a mel-ﬁlterbank to linear spectrograms.\n\nWe use International Phonetic Alphabet (IPA) sequences as\ninput to the prior encoder. We convert text sequences to IPA\nphoneme sequences using open-source software (Bernard,\n2021), and the converted sequences are interspersed with a\nblank token following the implementation of Glow-TTS.\n\n2.5.5. STOCHASTIC DURATION PREDICTOR\n\n3.3. Training\n\nThe stochastic duration predictor estimates the distribu-\ntion of phoneme duration from a conditional input htext.\nFor the efﬁcient parameterization of the stochastic dura-\ntion predictor, we stack residual blocks with dilated and\ndepth-separable convolutional layers. We also apply neural\nspline ﬂows (Durkan et al., 2019), which take the form of\ninvertible nonlinear transformations by using monotonic\nrational-quadratic splines, to coupling layers. Neural spline\nﬂows improve transformation expressiveness with a similar\nnumber of parameters compared to commonly used afﬁne\ncoupling layers. For the multi-speaker setting, we add a\nlinear layer that transforms speaker embedding and add it to\nthe input htext.\n\n3. Experiments\n\n3.1. Datasets\n\nWe conducted experiments on two different datasets. We\nused the LJ Speech dataset (Ito, 2017) for compari-\nson with other publicly available models and the VCTK\ndataset (Veaux et al., 2017) to verify whether our model\ncan learn and express diverse speech characteristics. The\nLJ Speech dataset consists of 13,100 short audio clips of a\nsingle speaker with a total length of approximately 24 hours.\nThe audio format is 16-bit PCM with a sample rate of 22\nkHz, and we used it without any manipulation. We ran-\ndomly split the dataset into a training set (12,500 samples),\nvalidation set (100 samples), and test set (500 samples). The\nVCTK dataset consists of approximately 44,000 short audio\nclips uttered by 109 native English speakers with various\naccents. The total length of the audio clips is approximately\n44 hours. The audio format is 16-bit PCM with a sample\nrate of 44 kHz. We reduced the sample rate to 22 kHz.\nWe randomly split the dataset into a training set (43,470\nsamples), validation set (100 samples), and test set (500\nsamples).\n\nThe networks are trained using the AdamW opti-\nmizer (Loshchilov & Hutter, 2019) with β1 = 0.8, β2 =\n0.99 and weight decay λ = 0.01. The learning rate decay is\nscheduled by a 0.9991/8 factor in every epoch with an initial\nlearning rate of 2 × 10−4. Following previous work (Ren\net al., 2021; Donahue et al., 2021), we adopt the windowed\ngenerator training, a method of generating only a part of\nraw waveforms to reduce the training time and memory\nusage during training. We randomly extract segments of\nlatent representations with a window size of 32 to feed to\nthe decoder instead of feeding entire latent representations\nand also extract the corresponding audio segments from the\nground truth raw waveforms as training targets. We use\nmixed precision training on 4 NVIDIA V100 GPUs. The\nbatch size is set to 64 per GPU and the model is trained up\nto 800k steps.\n\n3.4. Experimental Setup for Comparison\n\nWe compared our model with the best publicly available\nmodels. We used Tacotron 2, an autoregressive model, and\nGlow-TTS, a ﬂow-based non-autoregressive model, as ﬁrst\nstage models and HiFi-GAN as a second stage model. We\nused their public implementations and pre-trained weights.3\nSince a two-stage TTS system can theoretically achieve\nhigher synthesis quality through sequential training, we in-\ncluded the ﬁne-tuned HiFi-GAN up to 100k steps with the\npredicted outputs from the ﬁrst stage models. We empiri-\ncally found that ﬁne-tuning HiFi-GAN with the generated\nmel-spectrograms from Tacotron 2 under teacher-forcing\nmode, led to better quality for both Tacotron 2 and Glow-\nTTS than ﬁne-tuning with the generated mel-spectrograms\nfrom Glow-TTS, so we appended the better ﬁne-tuned HiFi-\n\n3The implementations are as follows:\n\nTacotron 2 : https://github.com/NVIDIA/tacotron2\nGlow-TTS : https://github.com/jaywalnut310/glow-tts\nHiFi-GAN : https://github.com/jik876/hiﬁ-gan\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nGAN to both Tacotron 2 and Glow-TTS.\n\nAs each model has a degree of randomness during sampling,\nwe ﬁxed hyper-parameters that controls the randomness of\neach model throughout our experiments. The probability of\ndropout in the pre-net of Tactron 2 was set to 0.5. For Glow-\nTTS, the standard deviation of the prior distribution was set\nto 0.333. For VITS, the standard deviation of input noise\nof the stochastic duration predictor was set to 0.8 and we\nmultiplied a scale factor of 0.667 to the standard deviation\nof the prior distribution.\n\n4. Results\n\n4.1. Speech Synthesis Quality\n\nWe conducted crowd-sourced MOS tests to evaluate the\nquality. Raters listened to randomly selected audio samples,\nand rated their naturalness on a 5 point scale from 1 to 5.\nRaters were allowed to evaluate each audio sample once,\nand we normalized all the audio clips to avoid the effect\nof amplitude differences on the score. All of the quality\nassessments in this work were conducted in this manner.\n\nThe evaluation results are shown in Table 1. VITS outper-\nforms other TTS systems and achieves a similar MOS to that\nof ground truth. The VITS (DDP), which employs the same\ndeterministic duration predictor architecture used in Glow-\nTTS rather than the stochastic duration predictor, scores the\nsecond-highest among TTS systems in the MOS evaluation.\nThese results imply that 1) the stochastic duration predictor\ngenerates more realistic phoneme duration than the deter-\nministic duration predictor and 2) our end-to-end training\nmethod is an effective way to make better samples than\nother TTS models even if maintaining the similar duration\npredictor architecture.\n\nthe normalizing ﬂow in the prior encoder results in a 1.52\nMOS decrease from the baseline, demonstrating that the\nprior distribution’s ﬂexibility signiﬁcantly inﬂuences the\nsynthesis quality. Replacing the linear-scale spectrogram\nfor posterior input with the mel-spectrogram results in a\nquality degradation (-0.19 MOS), indicating that the high-\nresolution information is effective for VITS in improving\nthe synthesis quality.\n\nTable 2. MOS comparison in the ablation studies.\n\nModel\n\nMOS (CI)\n\n4.50 (±0.06)\nGround Truth\n4.50 (±0.06)\nBaseline\nwithout Normalizing Flow 2.98 (±0.08)\n4.31 (±0.08)\nwith Mel-spectrogram\n\n4.2. Generalization to Multi-Speaker Text-to-Speech\n\nTo verify that our model can learn and express diverse\nspeech characteristics, we compared our model to Tacotron\n2, Glow-TTS and HiFi-GAN, which showed the ability to\nextend to multi-speaker speech synthesis (Jia et al., 2018;\nKim et al., 2020; Kong et al., 2020). We trained the mod-\nels on the VCTK dataset. We added speaker embedding\nto our model as described in Section 2.5. For Tacotron\n2, we broadcasted speaker embedding and concatenated it\nwith the encoder output, and for Glow-TTS, we applied the\nglobal conditioning following the previous work. The eval-\nuation method is the same as that described in Section 4.1.\nAs shown in Table 3, our model achieves a higher MOS\nthan the other models. This demonstrates that our model\nlearns and expresses various speech characteristics in an\nend-to-end manner.\n\nTable 1. Comparison of evaluated MOS with 95% conﬁdence in-\ntervals on the LJ Speech dataset.\n\nTable 3. Comparison of evaluated MOS with 95% conﬁdence in-\ntervals on the VCTK dataset.\n\nModel\n\nGround Truth\nTacotron 2 + HiFi-GAN\nTacotron 2 + HiFi-GAN (Fine-tuned)\nGlow-TTS + HiFi-GAN\nGlow-TTS + HiFi-GAN (Fine-tuned)\nVITS (DDP)\nVITS\n\nMOS (CI)\n\n4.46 (±0.06)\n3.77 (±0.08)\n4.25 (±0.07)\n4.14 (±0.07)\n4.32 (±0.07)\n4.39 (±0.06)\n4.43 (±0.06)\n\nModel\n\nGround Truth\nTacotron 2 + HiFi-GAN\nTacotron 2 + HiFi-GAN (Fine-tuned)\nGlow-TTS + HiFi-GAN\nGlow-TTS + HiFi-GAN (Fine-tuned)\nVITS\n\nMOS (CI)\n\n4.38 (±0.07)\n3.14 (±0.09)\n3.19 (±0.09)\n3.76 (±0.07)\n3.82 (±0.07)\n4.38 (±0.06)\n\nWe conducted an ablation study to demonstrate the effec-\ntiveness of our methods, including the normalized ﬂow in\nthe prior encoder and linear-scale spectrogram posterior in-\nput. All models in the ablation study were trained up to\n300k steps. The results are shown in Table 2. Removing\n\n4.3. Speech Variation\n\nWe veriﬁed how many different lengths of speech the\nstochastic duration predictor produces, and how many dif-\nferent speech characteristics the synthesized samples have.\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\n(a) Comparison of sample duration. Glow-TTS only provides a\nsingle value due to the deterministic duration predictor.\n\n(a) VITS\n\n(b) Tacotron 2\n\n(c) Glow-TTS\n\n(b) Comparison of sample duration in different speakers.\n\nFigure 2. Sample duration in seconds on (a) the LJ Speech dataset\nand (b) the VCTK dataset.\n\nSimilar to Valle et al. (2021), all samples here were gen-\nerated from a sentence “How much variation is there?”.\nFigure 2a shows histograms of the lengths of 100 generated\nutterances from each model. While Glow-TTS generates\nonly ﬁxed-length utterances due to the deterministic du-\nration predictor, samples from our model follow a similar\nlength distribution to that of Tacotron 2. Figure 2b shows the\nlengths of 100 utterances generated with each of ﬁve speaker\nidentities from our model in the multi-speaker setting, im-\nplying that the model learns the speaker-dependent phoneme\nduration. F0 contours of 10 utterances extracted with the\nYIN algorithm (De Cheveign´e & Kawahara, 2002) in Fig-\nure 3 shows that our model generates speech with diverse\npitches and rhythms, and ﬁve samples generated with each\nof different speaker identities in Figure 3d demonstrates\nour model expresses very different lengths and pitches of\nspeech for each speaker identity. Note that Glow-TTS could\nincrease the diversity of pitch by increasing the standard\ndeviation of the prior distribution, but on the contrary, it\ncould lower the synthesis quality.\n\n4.4. Synthesis Speed\n\nWe compared the synthesis speed of our model with a paral-\nlel two-stage TTS system, Glow-TTS and HiFi-GAN. We\nmeasured the synchronized elapsed time over the entire pro-\ncess to generate raw waveforms from phoneme sequences\n\n(d) VITS (multi-speaker)\n\nFigure 3. Pitch tracks for the utterance “How much variation is\nthere?”. Samples are generated from (a) VITS, (b) Tacotron 2, and\n(c) Glow-TTS in the single speaker setting and from (d) VITS in\nthe multi-speaker setting.\n\nwith 100 sentences randomly selected from the test set of the\nLJ Speech dataset. We used a single NVIDIA V100 GPU\nwith a batch size of 1. The results are shown in Table 4.\nSince our model does not require modules for generating\npredeﬁned intermediate representations, its sampling efﬁ-\nciency and speed are greatly improved.\n\n5. Related Work\n\n5.1. End-to-End Text-to-Speech\n\nCurrently, neural TTS models with a two-stage pipeline can\nsynthesize human-like speech (Oord et al., 2016; Ping et al.,\n2018; Shen et al., 2018). However, they typically require\nvocoders trained or ﬁne-tuned with ﬁrst stage model output,\nwhich causes training and deployment inefﬁciency. They\nare also unable to reap the potential beneﬁts of an end-to-\nend approach that can use learned hidden representations\nrather than predeﬁned intermediate features.\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nTable 4. Comparison of the synthesis speed. Speed of n kHz means\nthat the model can generate n×1000 raw audio samples per second.\nReal-time means the synthesis speed over real-time.\n\nModel\n\nSpeed (kHz) Real-time\n\nGlow-TTS + HiFi-GAN\nVITS\nVITS (DDP)\n\n606.05\n1480.15\n2005.03\n\n×27.48\n×67.12\n×90.93\n\nRecently, single-stage end-to-end TTS models have been\nproposed to tackle the more challenging task of generat-\ning raw waveforms, which contain richer information (e.g.,\nhigh-frequency response and phase) than mel-spectrograms,\ndirectly from text. FastSpeech 2s (Ren et al., 2021) is an\nextension of FastSpeech 2 that enables end-to-end parallel\ngeneration by adopting adversarial training and an auxil-\niary mel-spectrogram decoder that helps learn text repre-\nsentations. However, to resolve the one-to-many problem,\nFastSpeech 2s must extract phoneme duration, pitch, and\nenergy from speech used as input conditions in training.\nEATS (Donahue et al., 2021) employs adversarial training\nas well and a differentiable alignment scheme. To resolve\nthe length mismatch problem between generated and target\nspeech, EATS adopts soft dynamic time warping loss that is\ncalculated by dynamic programming. Wave Tacotron (Weiss\net al., 2020) combines normalizing ﬂows with Tacotron 2\nfor an end-to-end structure but remains autoregressive. The\naudio quality of all the aforementioned end-to-end TTS\nmodels is less than that of two-stage models.\n\nUnlike the aforementioned end-to-end models, by utiliz-\ning a conditional VAE, our model 1) learns to synthesize\nraw waveforms directly from text without requiring addi-\ntional input conditions, 2) uses a dynamic programming\nmethod, MAS, to search the optimal alignment rather than\nto calculate loss, 3) generates samples in parallel, and 4)\noutperforms the best publicly available two-stage models.\n\n5.2. Variational Autoencoders\n\nVAEs (Kingma & Welling, 2014) are one of the most widely\nused likelihood-based deep generative models. We adopt a\nconditional VAE to a TTS system. A conditional VAE is a\nconditional generative model where the observed conditions\nmodulate the prior distribution of latent variables used to\ngenerate outputs. In speech synthesis, Hsu et al. (2019) and\nZhang et al. (2019) combine Tacotron 2 and VAEs to learn\nspeech style and prosody. BVAE-TTS (Lee et al., 2021) gen-\nerates mel-spectrograms in parallel based on a bidirectional\nVAE (Kingma et al., 2016). Unlike the previous works that\napplied VAEs to ﬁrst stage models, we adopt a VAE to a\nparallel end-to-end TTS system.\n\nRezende & Mohamed (2015), Chen et al. (2017) and Ziegler\n& Rush (2019) improve VAE performance by enhancing\nthe expressive power of prior and posterior distribution with\nnormalizing ﬂows. To improve the representation power\nof the prior distribution, we add normalizing ﬂows to our\nconditional prior network, leading to the generation of more\nrealistic samples.\n\nSimilar to our work, Ma et al. (2019) proposed a condi-\ntional VAE with normalizing ﬂows in a conditional prior\nnetwork for non-autoregressive neural machine translation,\nFlowSeq. However, the fact that our model can explicitly\nalign a latent sequence with the source sequence differs from\nFlowSeq, which needs to learn implicit alignment through\nattention mechanisms. Our model removes the burden of\ntransforming the latent sequence into standard normal ran-\ndom variables by matching the latent sequence with the\ntime-aligned source sequence via MAS, which allows for\nsimpler architecture of normalizing ﬂows.\n\n5.3. Duration Prediction in Non-Autoregressive\n\nText-to-Speech\n\nAutoregressive TTS models (Taigman et al., 2018; Shen\net al., 2018; Valle et al., 2021) generate diverse speech with\ndifferent rhythms through their autoregressive structure and\nseveral tricks including maintaining dropout probability dur-\ning inference and priming (Graves, 2013). Parallel TTS\nmodels (Ren et al., 2019; Peng et al., 2020; Kim et al.,\n2020; Ren et al., 2021; Lee et al., 2021), on the other hand,\nhave been relied on deterministic duration prediction. It is\nbecause parallel models have to predict target phoneme dura-\ntion or the total length of target speech in one feed-forward\npath, which makes it hard to capture the correlated joint\ndistribution of speech rhythms. In this work, we suggest a\nﬂow-based stochastic duration predictor that learns the joint\ndistribution of the estimated phoneme duration, resulting in\nthe generation of diverse speech rhythms in parallel.\n\n6. Conclusion\n\nIn this work, we proposed a parallel TTS system, VITS, that\ncan learn and generate in an end-to-end manner. We further\nintroduced the stochastic duration predictor to express di-\nverse rhythms of speech. The resulting system synthesizes\nnatural sounding speech waveforms directly from text, with-\nout having to go through predeﬁned intermediate speech\nrepresentations. Our experimental results show that our\nmethod outperforms two-stage TTS systems and achieves\nclose to human quality. We hope the proposed method will\nbe used in many speech synthesis tasks, where two-stage\nTTS systems have been used, to achieve performance im-\nprovement and enjoy the simpliﬁed training procedure. We\nalso want to point out that even though our method inte-\ngrates two separated generative pipelines in TTS systems,\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nthere remains a problem of text preprocessing. Investigating\nself-supervised learning of language representations could\nbe a possible direction for removing the text preprocess-\ning step. We will release our source-code and pre-trained\nmodels to facilitate research in plenty of future directions.\n\nAcknowledgements\n\nWe would like to thank Sungwon Lyu, Bokyung Son,\nSunghyo Chung, and Jonghoon Mo for helpful discussions\nand advice.\n\nReferences\n\nGraves, A. Generating sequences with recurrent neural\n\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\n\nHo, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P.\nFlow++: Improving ﬂow-based generative models with\nIn\nvariational dequantization and architecture design.\nInternational Conference on Machine Learning, pp. 2722–\n2730. PMLR, 2019.\n\nHsu, W.-N., Zhang, Y., Weiss, R., Zen, H., Wu, Y., Cao,\nY., and Wang, Y. Hierarchical Generative Modeling for\nControllable Speech Synthesis. In International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=rygkk305YQ.\n\nBernard, M. Phonemizer. https://github.com/\n\nbootphon/phonemizer, 2021.\n\nIto, K. The LJ Speech Dataset. https://keithito.\n\ncom/LJ-Speech-Dataset/, 2017.\n\nBi´nkowski, M., Donahue, J., Dieleman, S., Clark, A., Elsen,\nE., Casagrande, N., Cobo, L. C., and Simonyan, K. High\nFidelity Speech Synthesis with Adversarial Networks. In\nInternational Conference on Learning Representations,\n2019.\n\nJia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren,\nF., Chen, Z., Nguyen, P., Pang, R., Lopez-Moreno, I.,\net al. Transfer Learning from Speaker Veriﬁcation to\nMultispeaker Text-To-Speech Synthesis. In Advances in\nNeural Information Processing Systems, 2018.\n\nChen, J., Lu, C., Chenli, B., Zhu, J., and Tian, T. Vﬂow:\nMore expressive generative ﬂows with variational data\naugmentation. In International Conference on Machine\nLearning, pp. 1660–1669. PMLR, 2020.\n\nChen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhari-\nwal, P., Schulman, J., Sutskever, I., and Abbeel, P.\nVariational lossy autoencoder. 2017. URL https:\n//openreview.net/forum?id=BysvGP5ee.\n\nDe Cheveign´e, A. and Kawahara, H. Yin, a fundamental\nfrequency estimator for speech and music. The Journal\nof the Acoustical Society of America, 111(4):1917–1930,\n2002.\n\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti-\nmation using Real NVP. In International Conference on\nLearning Representations, 2017.\n\nDonahue, J., Dieleman, S., Binkowski, M., Elsen, E., and\nSimonyan, K. End-to-end Adversarial Text-to-Speech. In\nInternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=rsf1z-JSj87.\n\nDurkan, C., Bekasov, A., Murray, I., and Papamakarios, G.\nNeural Spline Flows. In Advances in Neural Information\nProcessing Systems, pp. 7509–7520, 2019.\n\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative Adversarial Nets. Advances in Neural\nInformation Processing Systems, 27:2672–2680, 2014.\n\nKalchbrenner, N., Elsen, E., Simonyan, K., Noury, S.,\nCasagrande, N., Lockhart, E., Stimberg, F., Oord, A.,\nDieleman, S., and Kavukcuoglu, K. Efﬁcient neural au-\ndio synthesis. In International Conference on Machine\nLearning, pp. 2410–2419. PMLR, 2018.\n\nKim, J., Kim, S., Kong, J., and Yoon, S. Glow-TTS: A Gen-\nerative Flow for Text-to-Speech via Monotonic Align-\nment Search. Advances in Neural Information Processing\nSystems, 33, 2020.\n\nKingma, D. P. and Welling, M. Auto-Encoding Variational\nBayes. In International Conference on Learning Repre-\nsentations, 2014.\n\nKingma, D. P., Salimans, T., Jozefowicz, R., Chen, X.,\nSutskever, I., and Welling, M. Improved variational in-\nference with inverse autoregressive ﬂow. Advances in\nNeural Information Processing Systems, 29:4743–4751,\n2016.\n\nKong, J., Kim, J., and Bae, J. HiFi-GAN: Generative Ad-\nversarial networks for Efﬁcient and High Fidelity Speech\nSynthesis. Advances in Neural Information Processing\nSystems, 33, 2020.\n\nKumar, K., Kumar, R., de Boissiere, T., Gestin, L., Teoh,\nW. Z., Sotelo, J., de Br´ebisson, A., Bengio, Y., and\nCourville, A. C. MelGAN: Generative Adversarial Net-\nworks for Conditional waveform synthesis. volume 32,\npp. 14910–14921, 2019.\n\nLarsen, A. B. L., Sønderby, S. K., Larochelle, H., and\nWinther, O. Autoencoding beyond pixels using a learned\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nsimilarity metric. In International Conference on Ma-\nchine Learning, pp. 1558–1566. PMLR, 2016.\n\nLee, Y., Shin, J., and Jung, K. Bidirectional Variational\nInference for Non-Autoregressive Text-to-speech.\nIn\nInternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=o3iritJHLfO.\n\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. Neural speech\nsynthesis with transformer network. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 33,\npp. 6706–6713, 2019.\n\nLoshchilov, I. and Hutter, F. Decoupled Weight Decay\nRegularization. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.\nnet/forum?id=Bkg6RiCqY7.\n\nMa, X., Zhou, C., Li, X., Neubig, G., and Hovy, E. Flowseq:\nNon-autoregressive conditional sequence generation with\ngenerative ﬂow. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 4273–4283,\n2019.\n\nMao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and\nPaul Smolley, S. Least squares generative adversarial\nnetworks. In Proceedings of the IEEE international con-\nference on computer vision, pp. 2794–2802, 2017.\n\nMiao, C., Liang, S., Chen, M., Ma, J., Wang, S., and Xiao,\nJ. Flow-TTS: A non-autoregressive network for text to\nspeech based on ﬂow. In ICASSP 2020-2020 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 7209–7213. IEEE, 2020.\n\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,\nand Kavukcuoglu, K. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, 2016.\n\nPeng, K., Ping, W., Song, Z., and Zhao, K. Non-\nIn International\nautoregressive neural text-to-speech.\nConference on Machine Learning, pp. 7586–7598.\nPMLR, 2020.\n\nPing, W., Peng, K., Gibiansky, A., Arik, S. O., Kannan, A.,\nNarang, S., Raiman, J., and Miller, J. Deep Voice 3: 2000-\nSpeaker Neural Text-to-Speech. In International Confer-\nence on Learning Representations, 2018. URL https:\n//openreview.net/forum?id=HJtEm4p6Z.\n\nPrenger, R., Valle, R., and Catanzaro, B. Waveglow: A\nﬂow-based generative network for speech synthesis. In\nICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp.\n3617–3621. IEEE, 2019.\n\nRen, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and\nLiu, T.-Y. FastSpeech: Fast, Robust and Controllable\nText to Speech. volume 32, pp. 3171–3180, 2019.\n\nRen, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu,\nT.-Y. FastSpeech 2: Fast and High-Quality End-to-End\nText to Speech. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.\nnet/forum?id=piLPYqxtWuA.\n\nRezende, D. and Mohamed, S. Variational inference with\nnormalizing ﬂows. In International Conference on Ma-\nchine Learning, pp. 1530–1538. PMLR, 2015.\n\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-Attention\nwith Relative Position Representations. In Proceedings\nof the 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers), pp.\n464–468, 2018.\n\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N.,\nYang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R.,\net al. Natural tts synthesis by conditioning wavenet on\nmel spectrogram predictions. In 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 4779–4783. IEEE, 2018.\n\nTaigman, Y., Wolf, L., Polyak, A., and Nachmani, E.\nVoiceloop: Voice Fitting and Synthesis via a Phono-\nlogical Loop. In International Conference on Learning\nRepresentations, 2018. URL https://openreview.\nnet/forum?id=SkFAWax0-.\n\nValle, R., Shih, K. J., Prenger, R., and Catanzaro,\nB. Flowtron: an Autoregressive Flow-based Gener-\nIn In-\native Network for Text-to-Speech Synthesis.\nternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=Ig53hpHxS4.\n\nvan den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural\ndiscrete representation learning. In Advances in Neural\nInformation Processing Systems, pp. 6309–6318, 2017.\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is All you Need. Advances in Neural Information\nProcessing Systems, 30:5998–6008, 2017.\n\nVeaux, C., Yamagishi, J., MacDonald, K., et al. CSTR\nVCTK corpus: English multi-speaker corpus for CSTR\nvoice cloning toolkit. University of Edinburgh. The Cen-\ntre for Speech Technology Research (CSTR), 2017.\n\nWeiss, R. J., Skerry-Ryan, R., Battenberg, E., Mariooryad,\nS., and Kingma, D. P. Wave-Tacotron: Spectrogram-\nfree end-to-end text-to-speech synthesis. arXiv preprint\narXiv:2011.03568, 2020.\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nZeng, Z., Wang, J., Cheng, N., Xia, T., and Xiao, J. Aligntts:\nEfﬁcient feed-forward text-to-speech system without ex-\nplicit alignment. In ICASSP 2020-2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 6714–6718. IEEE, 2020.\n\nZhang, Y.-J., Pan, S., He, L., and Ling, Z.-H. Learning\nlatent representations for style control and transfer in end-\nto-end speech synthesis. In ICASSP 2019-2019 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pp. 6945–6949. IEEE, 2019.\n\nZiegler, Z. and Rush, A. Latent normalizing ﬂows for dis-\ncrete sequences. In International Conference on Machine\nLearning, pp. 7673–7682. PMLR, 2019.\n\n\x0cSupplementary Material of\nConditional Variational Autoencoder with Adversarial Learning for\nEnd-to-End Text-to-Speech\n\nA. Monotonic Alignment Search\n\nWe present pseudocode for MAS in Figure 4. Although we search the alignment which maximizes the ELBO not the exact\nlog-likelihood of data, we can use the MAS implementation of Glow-TTS as described in Section 2.2.1.\n\ndef monotonic_alignment_search(value):\n\n"""Returns the most likely alignment for the given log-likelihood matrix.\nArgs:\n\nvalue: the log-likelihood matrix. Its (i, j)-th entry contains\nthe log-likelihood of the j-th latent variable\nfor the given i-th prior mean and variance:\n.. math::\n\nvalue_{i,j} = log N(f(z)_{j}; \\mu_{i}, \\sigma_{i})\n(dtype=float, shape=[text_length, latent_variable_length])\n\nReturns:\n\npath: the most likely alignment.\n(dtype=float, shape=[text_length, latent_variable_length])\n\n"""\nt_x, t_y = value.shape # [text_length, letent_variable_length]\npath = zeros([t_x, t_y])\n\n# A cache to store the log-likelihood for the most likely alignment so far.\nQ = -INFINITY * ones([t_x, t_y])\n\nfor y in range(t_y):\n\nfor x in range(max(0, t_x + y - t_y), min(t_x, y + 1)):\n\nif y == 0: # Base case. If y is 0, the possible x value is only 0.\n\nQ[x, 0] = value[x, 0]\n\nelse:\n\nif x == 0:\n\nv_prev = -INFINITY\n\nelse:\n\nv_prev = Q[x-1, y-1]\n\nv_cur = Q[x, y-1]\nQ[x, y] = value[x, y] + max(v_prev, v_cur)\n\n# Backtrack from last observation.\nindex = t_x - 1\nfor y in range(t_y - 1, -1, -1):\n\npath[index, y] = 1\nif index != 0 and (index == y or Q[index, y-1] < Q[index-1, y-1]):\n\nindex = index - 1\n\nreturn path\n\nFigure 4. Pseudocode for Monotonic Alignment Search.\n\nB. Model Conﬁgurations\n\nIn this section, we mainly describe the newly added parts of VITS as we followed conﬁgurations of Glow-TTS and\nHiFi-GAN for several parts of our model: we use the same transformer encoder and WaveNet residual blocks as those of\nGlow-TTS; our decoder and the multi-period discriminator is the same as the generator and multi-period discriminator of\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nHiFi-GAN, respectively, except that we use different input dimension for the decoder and append a sub-discriminator.\n\nB.1. Prior Encoder and Posterior Encoder\n\nThe normalizing ﬂow in the prior encoder is a stack of four afﬁne coupling layers, each coupling layer consisting of four\nWaveNet residual blocks. As we restrict the afﬁne coupling layers to be volume-preserving transformations, the coupling\nlayers do not produce scale parameters.\n\nThe posterior encoder, consisting of 16 WaveNet residual blocks, takes linear-scale log magnitude spectrograms and produce\nlatent variables with 192 channels.\n\nB.2. Decoder and Discriminator\n\nThe input of our decoder is latent variables generated from the prior or posterior encoders, so the input channel size of the\ndecoder is 192. For the last convolutional layer of the decoder, we remove a bias parameter, as it causes unstable gradient\nscales during mixed precision training.\n\nFor the discriminator, HiFi-GAN uses the multi-period discriminator containing ﬁve sub-discriminators with periods\n[2, 3, 5, 7, 11] and the multi-scale discriminator containing three sub-discriminators. To improve training efﬁciency, we\nleave only the ﬁrst sub-discriminator of the multi-scale discriminator that operates on raw waveforms and discard two\nsub-discriminators operating on average-pooled waveforms. The resultant discriminator can be seen as the multi-period\ndiscriminator with periods [1, 2, 3, 5, 7, 11].\n\n(a) Training procedure\n\n(b) Inference procedure\n\n(c) Dilated and depth-wise separable convo-\nlutional residual block\n\nFigure 5. Block diagram depicting (a) training procedure and (b) inference procedure of the stochastic duration predictor. The main\nbuilding block of the stochastic duration predictor is (c) the dilated and depth-wise separable convolutional residual block.\n\nB.3. Stochastic Duration Predictor\n\nFigures 5a and 5b show the training and inference procedures of the stochastic duration predictor, respectively. The main\nbuilding block of the stochastic duration predictor is the dilated and depth-wise separable convolutional (DDSConv) residual\nblock as in Figure 5c. Each convolutional layer in DDSConv blocks is followed by a layer normalization layer and GELU\nactivation function. We choose to use dilated and depth-wise separable convolutional layers for improving parameter\nefﬁciency while maintaining large receptive ﬁeld size.\n\nThe posterior encoder and normalizing ﬂow module in the duration predictor are ﬂow-based neural networks and have the\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nsimilar architecture. The difference is that the posterior encoder transforms a Gaussian noise sequence into two random\nvariables ν and u to express the approximate posterior distribution qφ(u, ν|d, ctext), and the normalizing ﬂow module\ntransforms d − u and ν into a Gaussian noise sequence to express the log-likelihood of the augmented and dequantized data\nlog pθ(d − u, ν|ctext) as described in Section 2.2.2.\n\nAll input conditions are processed through condition encoders, each consisting of two 1x1 convolutional layers and a\nDDSConv residual block. The posterior encoder and normalizing ﬂow module have four coupling layers of neural spline\nﬂows. Each coupling layer ﬁrst processes input and input conditions through a DDSConv block and produces 29-channel\nparameters that are used to construct 10 rational-quadratic functions. We set the hidden dimension of all coupling layers and\ncondition encoders to 192. Figure 6a and 6b show the architecture of a condition encoder and a coupling layer used in the\nstochastic duration predictor.\n\n(a) Condition encoder in the stochastic duration predictor\n\n(b) Coupling layer in the stochastic duration predictor\n\nFigure 6. The architecture of (a) condition encoder and (b) coupling layer used in the stochastic duration predictor.\n\nC. Side-by-Side Evaluation\n\nWe conducted 7-point Comparative Mean Opinion Score (CMOS) evaluation between VITS and the ground truth through\n500 ratings on 50 items. Our model achieved -0.106 and -0.270 CMOS on the LJ Speech and the VCTK datasets, respectively,\nas in Table 5. It indicates that even though our model outperforms the best publicly available TTS system, Glow-TTS and\nHiFi-GAN, and achieves a comparable score to ground truth in MOS evaluation, there remains a small preference of raters\ntowards the ground truth over our model.\n\nTable 5. Evaluated CMOS of VITS compared to the ground truth.\n\nDataset\n\nCMOS\n\nLJ Speech\nVCTK\n\n-0.106\n-0.262\n\nD. Voice Conversion\n\nIn the multi-speaker setting, we do not provide speaker identities into the text encoder, which makes the latent variables\nestimated from the text encoder learn speaker-independent representations. Using the speaker-independent representations,\nwe can transform an audio recording of one speaker into a voice of another speaker. For a given speaker identity s and an\nutterance of the speaker, we can attain a linear spectrogram xlin from the corresponding utterance audio. We can transform\nxlin into a speaker-independent representation e through the posterior encoder and the normalizing ﬂow in the prior encoder:\n\nz ∼ qφ(z|xlin, s)\ne = fθ(z|s)\n\n(12)\n\n(13)\n\n\x0cConditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech\n\nThen, we can synthesize a voice ˆy of a target speaker identity ˆs from the representation e through the inverse transformation\nof the normalizing ﬂow f −1\n\nand decoder G:\n\nθ\n\nˆy = G(f −1\n\nθ\n\n(e|ˆs)|ˆs)\n\n(14)\n\nLearning speaker-independent representations and using it for voice conversion can be seen as an extension of the voice\nconversion method proposed in Glow-TTS. Our voice conversion method provides raw waveforms rather than mel-\nspectrograms as in Glow-TTS. The voice conversion results are presented in Figure 7. It shows a similar trend of pitch\ntracks with different pitch levels.\n\nFigure 7. Pitch tracks of a ground truth sample and the corresponding voice conversion samples with different speaker identities.\n\n\x0c'